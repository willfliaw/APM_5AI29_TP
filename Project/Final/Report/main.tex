\documentclass[12pt,a4paper]{article}

% Quelques options d'encodage, de langues et de format du document
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[top=2cm, bottom=3cm, left=1.75cm, right=1.75cm]{geometry}
\usepackage{setspace}

\usepackage{graphicx} % Pour la commande "\includegraphics"
\usepackage[ % Modified parameters
    bookmarks,
    colorlinks,
    citecolor=black,
    urlcolor=blue,
    linkcolor=black,
    pdfpagemode=UseNone
]{hyperref} % Pour la commande "\url"

\pagenumbering{arabic}

% ------------------------------------------------------------------
% CITATION MANAGEMENT
% ------------------------------------------------------------------

% APA Style
\usepackage{cite}
%\bibliographystyle{apalike}
\bibliographystyle{ieeetr}

\renewcommand\refname{Bibliography}

% Proper formatting and line breaking for URLs
\usepackage{url}

% Change "References" to "Bibliography"
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{\section*{Bibliography}}{}{}

% ------------------------------------------------------------------
% SUBFIGURE MANAGEMENT
% ------------------------------------------------------------------

\usepackage[caption=false]{subfig}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.18}
\usepackage{booktabs}

% ------------------------------------------------------------------
% COLORED TEXT MANAGEMENT
% ------------------------------------------------------------------

\usepackage{xcolor}
\usepackage{todonotes}
\setuptodonotes{inline=always}
\setlength {\marginparwidth }{2cm}

% ------------------------------------------------------------------
% CODE MANAGEMENT
% ------------------------------------------------------------------

\usepackage{listings}
\usepackage{cleveref}

% needs sheel escape activated
% --shell-escape
%\usetikzlibrary{external}
%\tikzexternalize[prefix=figures_tikz_compiled/]

\lstdefinelanguage{Cypher}{
    morekeywords={
        MATCH, RETURN, WHERE, CREATE, DELETE, SET, MERGE, DETACH,
        WITH, LIMIT, SKIP, ORDER, BY, DESC, ASC, OPTIONAL, CALL
    },
    sensitive=true,
    morecomment=[l]{//},
    morecomment=[s]{/*}{*/},
    morestring=[b]',
    morestring=[b]"
}

% Listing style for better readability
\lstset{
    language=Cypher,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{orange},
    commentstyle=\color{gray},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    frame=single
}

\usepackage{xspace}
\newcommand{\modelministral}{Ministral-8B\xspace}
\newcommand{\modelalpaca}{Alpaca-7B\xspace}
\newcommand{\modeldeepseek}{R1-Distill-8B\xspace}


\begin{document}

\begin{center}
    \begin{tabular}{|p{0.2\textwidth}|p{0.75\textwidth}|}
        \hline
        {
            \vspace{0cm} % without it, bugs, don't know why?
            \centerline{\includegraphics[width=\linewidth]{./images/tp-ipp}}
        }
         & {
                \vspace{0cm} % same here
                \centering
                \large
                {\hfill February, 2025}

                \vspace*{.5cm}
                \textbf{APM\_5AI29\_TP}

                \vspace*{.5cm}
                \setstretch{1.5}
                {\Large\textbf{Language Models and Structured Data}}

                \vspace*{.5cm}
                Final Project Report

                \vspace*{1cm}
        }    \\
        \hline
    \end{tabular}
\end{center}

\noindent Acronym of the Team: AWESome\\
Names: Mochamad Ardiansyah Nurgaha; William Liaw; Eddie Groh; Sriraam Appakutti Palani

    {\centering\rule{\linewidth}{.5pt}}

\begin{center}
    \section*{Knowledge Graph Completion}
\end{center}

\todo{Maybe abstract?}

% ---------------------------------------------------------
%
% PROBLEM STATEMENT
%
% ---------------------------------------------------------


\section{Problem Statement}\label{sec:problem_statement}
\todo{Define precisely what we are doing (predicting head/tail or just the relation?). Maybe Include a small example to illustrate the problem? Should not be necessary}
\todo{Be mathematically rigierous}

% Knowledge Graph Completion (KGC) is a fundamental task in artificial intelligence aimed at addressing the inherent incompleteness of knowledge graphs (KGs). KGs represent real-world facts as structured triples of the form $(h, r, t)$, where $h$ (head entity) and $t$ (tail entity) are connected by a relation $r$. Despite their widespread use in search engines, recommendation systems, and question-answering applications, KGs suffer from missing information, necessitating the development of KGC techniques to predict missing elements and improve KG utility.

% ---------------------------------------------------------
%
% BACKGROUND
%
% ---------------------------------------------------------


\section{Background}\label{sec:background}

Early KGC research centered on triple-based models using embedding methods to represent entities and relations in a continuous vector space.
TransE~\cite{bordes2013translating} models relations as translation operations, while DistMult~\cite{yang2014embedding} and ComplEx~\cite{trouillon2016complex} apply tensor factorization, the latter extending to complex-valued embeddings.

%Advancements introduced Graph Neural Networks (GNNs)~\cite{schlichtkrull2018modeling} to refine entity representations by aggregating structural information.
%However, these models struggle with long-tail entities due to sparse connectivity, limiting predictive performance.

%To overcome these limitations, text-based KGC leverages pre-trained language models (PLMs) to incorporate entity and relation descriptions. KG-BERT~\cite{yao2019kgbert} reframes KGC as a classification problem, encoding triples as text for distinguishing valid from invalid ones.
%Despite effectiveness, these methods require extensive fine-tuning and high computational costs, restricting scalability.

Large Language Models (LLMs) have revolutionized KGC by leveraging their broad internal knowledge and advanced reasoning capabilities.
This enables, in some cases, the ability to perform link prediction without even being trained on the target data, using techniques such as in-context learning and instruction tuning.
Unlike traditional knowledge graph (KG) models, which rely on structured embeddings, LLMs offer greater flexibility for zero-shot and few-shot predictions.
However, they often struggle with hallucination and structural misalignment with KG schemas.


We divide LLM-based KGC solutions into two main categories:

\textbf{Purely Prompt-Based Methods} exploit structured retrieval or textual descriptions alongside in-context learning, without modifying LLM parameters.
For instance, MPIKGC~\cite{xu2024mpikgc} enhances description-based KGC models by querying LLMs to expand entity representations, thereby improving relation understanding and capturing structural clues.
\todo{One paper here would be pretty good}
Finally, KICGPT~\cite{wei2023kicgpt} stands out by incorporating a triple-based retriever for preliminary ranking, then refining the candidate list through \emph{Knowledge Prompts} that encode KG structure directly into the LLM’s prompts.
Although KICGPT’s approach remains parameter-free, it injects structural awareness more explicitly than simpler prompt-based methods.

\textbf{Fine-Tuning-Based Methods} embed KG structure into the LLM through additional training.
For example, DIFT~\cite{liu2024dift} refines the model with discrimination instructions to better identify valid entities, while KC-GenRe~\cite{wang2024kcgenre} adopts a knowledge-constrained re-ranking technique, updating LLM weights to emphasize entity ranking.
%RPKGC~\cite{khalil2024rpkgc} fine-tunes a model for relation prediction, learning to infer plausible connections between entity pairs.
Finally, KoPA~\cite{qin2023kopa} integrates a \emph{Knowledge Prefix Adapter}, mapping KG embeddings into the LLM’s textual space to guide inference with structural signals.
Although KoPA modifies model weights, it keeps the KG structure directly accessible during generation and prediction.



% ---------------------------------------------------------
%
% METHODOLOGY
%
% ---------------------------------------------------------


\section{Methodology}\label{sec:methodology}
\todo{Introduce KICGPT metho as "prompt engie" maybe throw in a couple of formulas}
\todo{FANCY PICTURE OF ARCHITECTURE}
\todo{Extend or intro KoPA as "fine tuning"}
\todo{Another ultra fancy picture}
\todo{it might be hard to seperate this from the previous section, however it should be possible, if not we can also fuse them, but it might hinder readability since we keep jumping all over the place (cause other papers are required for background)}

In this project, we focus on LLM-based KGC and explore two different
state-of-the-art approaches: link prediction with KICGPT
and triple classification with KoPA.
While these tasks differ, they share a fundamental idea where both
methods use LLMs to assess and predict entity relationships
in a KG. KICGPT tackles link prediction using prompt engineering,
where demonstrations help guide the LLM in predicting missing entities.
Meanwhile, KoPA employs fine-tuning and embed structural information
as a prefix of LLM input performs triple classification to determine the
correctness of a given triple. However, at its core, KoPA also relies
on link prediction, as the LLM implicitly evaluates whether the
connection in the triple is plausible or not. The key difference
lies in execution: KICGPT predicts missing entities, while KoPA
predicts the validity of an existing triple by assessing whether
the inferred link is correct.

% Both approaches leverage LLMs to enhance KGC performance,
% but differ in their integration strategies. KICGPT adopts a prompt
% engineering approach, using demonstrations to guide LLM reasoning,
% while KoPA employs fine-tuning and embed structural information as a prefix of LLM input.

For link prediction task, given an incomplete triple in a KG \((h, r, ?)\) or \((?, r, t)\),
the goal is to predict the missing entity, either it is head \(h\)
or tail \(t\), where \(h, t \in E\) and \(r \in R\), E denotes set of
entities, R denotes set of relations. For the triple classification task,
the objective is to determine the correctness of the triple given an input
triple from a KG by comparing it with the ground truth.
The model that we used follows
KICGPT \cite{wei2023kicgpt} and KoPA \cite{qin2023kopa} with modification on the LLM backbone.

\subsection{KICGPT}
\label{sec:method:kicgpt}

% \subsection{Architecture}
% \label{sec:method:architecture}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/arc3.png}
    \caption{KICGPT Architecture \cite{wei2023kicgpt}}
    \label{fig:KICGPTarchitecture}
\end{figure}

KICGPT primarily covers three main components as can be seen in Figure
\ref{fig:KICGPTarchitecture}, which are a triple-based KGC retriever,
the Knowledge Prompt, and a LLM. For each query triple \((h, r, ?)\),
the KGC retriever retrieves the score of \((h, r, e)\) for each entity
\(e \in E\) in descending order
\(R_{retriever} = [e_1, e_2, ..., e_{|E|}]\).
Using prompt engineering in the Knowledge Prompt, LLM performs reranking
of the top \(m\) entities
\(R_{LLM} = [e^{'}_1, e^{'}_2, ..., e^{'}_m]\). Finally, KICGPT will
output final ranking of top m entities from the LLM and the rest of
the entities from the KGC retriever
\(R_{KICGPT} = [e^{'}_1, e^{'}_2, ..., e^{'}_m, e_{m+1}, ..., e_{|E|}]\)

\subsubsection{Knowledge Prompt}
Knowledge Prompt is introduced in KICGPT as in-context learning
strategy to provide context to the LLM by integrating part of the KG
into the demonstration. There are two pools of triples in the
demonstration, analogy pool \(D_a\) and supplement pool \(D_s\).
To help the LLM understand the query triple, the analogy pool contains
triples with the same relation \(r\) as the query \((h, r, ?)\),
\(D_a = \{(e^{'}, r, e^{"}) \in G_{train} \cup G_{valid} \mid e, e^{"} \in E\}\).
In addition, the supplement pool contains triples with one of its entity
(tail or head) is the same as the query's head \((h, r, ?)\),
\(D_s = \{(h, r, e^{'}) \in G_{train} \cup G_{valid} \mid r^{'} \in R, e^{'} \in E\} \cup
\{(e^{'}, r, h) \in G_{train} \cup G_{valid} \mid r^{'} \in R, e^{'} \in E\}\).
This is to provide additional information of the query's head to the LLM.

The ordering of the demonstration is important as it affects the LLM performance.
For the analogy pool, each entity starts with a zero counter.
A random triple from \(D_a\) is chosen first, increasing its entities'
counters by 1. Next, the triple with the lowest summed counter values
is selected, repeating until all triples are used. The final ordered
list is denoted \(L_a\). The supplement pool \(D_s\) provides additional context
for the query's head entity \(h\), prioritizing relevant demonstrations.
To achieve this, triples in \(D_s\) are ranked based on their BM25 scores,
which measure their textual similarity to the query. The final ordered list,
denoted \(L_s\), includes all triples from \(D_s\).

\subsubsection{Prompt Engineering}

To adapt structured triples into natural language input, KICGPT uses
a unified prompt template, ensuring consistent formatting for queries
and demonstrations. The interaction with the LLM follows a multi-round
process. First, the responsibility description stage clarifies the LLM's
role in ranking candidate answers. Next, in the question and demonstration
description stage, the query is presented along with an explanation of
two demonstration types: analogy-based and supplementary examples.

In the demonstration input stage, batches of demonstrations from the
analogy \(L_a\) and supplement \(L_s\) pools are provided, repeated
as much as the token limit allows to maximize knowledge inclusion.
Finally, during the final query and re-ranking stage, the LLM ranks
the top-m candidate entities, producing an ordered list \(R_{LLM}\),
which replaces the top-m results from the retriever to generate the
final answer.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/model.001.png}
    \caption{KoPA Architecture \cite{qin2023kopa}}
    \label{fig:KoPAarchitecture}
\end{figure}

\subsection{KoPA}
\label{sec:method:kopa}
Unlike KICGPT, KoPA integrates the structural information of
the KG into the LLM as an input. As can be seen in Figure \ref{fig:KoPAarchitecture},
KoPA comprises two main components: structural
embedding and knowledge prefix adapter.

\subsubsection{Structural Embedding}
KoPA captures structural information through self-supervised
structural embedding pre-training rather than adding neighborhood
text to the input. Each entity \(e \in E\)  and relation \(r \in R\)
is assigned a structural embedding, \(\mathbf{e} \in {R}^{d_e}\)
and \(\mathbf{r} \in {R}^{d_r}\), mapped into the
LLM's textual space.

To measure triple plausibility, KoPA follows the embedding-based
KGC paradigm by defining a score function \(F(h, r, t)\).
It optimizes structural embeddings using negative sampling
in a self-supervised pre-training process. By minimizing the
pre-training loss, the model captures subgraph structures and
relational patterns, aligning entity and relation representations
with KG structures.

\subsubsection{Knowledge Prefix Adapter}
After pre-training, the structural embeddings \((h, r, t)\) encode
KG information but exist in a different space from the LLM's textual
representations. To bridge this gap, a knowledge prefix adapter \(P\)
projects these embeddings into the LLM's token space by converting
them into virtual knowledge tokens (K):

\begin{equation}
    K = P(\mathbf{h}) \oplus P(\mathbf{r}) \oplus P(\mathbf{t})
\end{equation}

These tokens are added as prefix of the input sequence. It allows
the LLM to pay attention to structural information during fine-tuning
and inference. The adapter is trained while keeping embeddings
fixed to enable generalization to new triples, enrich textual
descriptions, and improve predictions.
% ---------------------------------------------------------
%
% EXPERIMENTS
%
% ---------------------------------------------------------


\section{Experiments}\label{sec:experiments}

\begin{figure}
    \centering
    \begin{tikzpicture}
        % Group plot configuration
        \begin{groupplot}[
                group style={
                        group size=3 by 1,       % 3 plots in 1 row
                        horizontal sep=0.3cm     % Adjust spacing between plots
                    },
                width=0.38\textwidth,        % Maximized plot width
                ymode=log,                   % Log-scale y-axis for ALL plots
                xmin=0, xmax=3,              % Unified x-axis range
                ymin=1e-3, ymax=1,           % Unified y-axis range across all plots
                xlabel=Epoch,
                xtick distance=0.5,          % Uniform tick spacing
                ytick distance=10,           % Log-scale tick separation
                minor y tick num=9,          % Minor tick marks for readability
            ]

            %------------- Plot (a) Alpaca -----------------
            \nextgroupplot[
                title=\modelalpaca,
                % Hide y-axis labels for alignment
                ylabel=Loss
            ]
            \addplot[smooth, thick, blue]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-Llama-2-7b-alpaca-cleaned-finetune.dat};

            \addplot[smooth, thick, cyan, dashed]
            table[x=epoch, y=grad-norm, col sep=space]
                {figures/raw-data/lora-Llama-2-7b-alpaca-cleaned-finetune.dat};


            %------------- Plot (b) DeepSeek ---------------
            \nextgroupplot[
                title=\modeldeepseek,
                yticklabels={,,}
            ]
            \addplot[smooth, thick, purple]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-DeepSeek-R1-Distill-Llama-8B-finetune.dat};

            \addplot[smooth, thick, violet, dashed]
            table[x=epoch, y=grad-norm, col sep=space]
                {figures/raw-data/lora-DeepSeek-R1-Distill-Llama-8B-finetune.dat};

            %------------- Plot (c) Loss Comparison -------------
            \nextgroupplot[
                title=Loss Comparison,
                yticklabels={,,}   % Hide y-axis tick labels for uniform look
            ]
            \addplot[smooth, thick, purple]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-DeepSeek-R1-Distill-Llama-8B-finetune.dat};

            \addplot[smooth, thick, blue]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-Llama-2-7b-alpaca-cleaned-finetune.dat};

        \end{groupplot}
    \end{tikzpicture}

    % Description of colors with more spacing in legend
    \vspace{0.2cm}
    {\centering
        \begin{tikzpicture}
            \node[draw, fill=blue, minimum width=0.4cm, minimum height=0.4cm] (A) {};
            \node[left=0.3cm of A] {\modelalpaca:};
            \node[right=0.05cm of A] {Loss};

            \node[draw, fill=cyan, minimum width=0.4cm, minimum height=0.4cm, right=1.5cm of A] (B) {};
            \node[right=0.05cm of B] {Gradient Norm};

            \node[draw, fill=purple, minimum width=0.4cm, minimum height=0.4cm, below=0.2cm of A] (C) {};
            \node[left=0.3cm of C] {\modeldeepseek:};
            \node[right=0.05cm of C] {Loss};

            \node[draw, fill=violet, minimum width=0.4cm, minimum height=0.4cm, right=1.5cm of C] (D) {};
            \node[right=0.05cm of D] {Gradient Norm};
        \end{tikzpicture}
    }

    \caption{Fine-tuning of \modelalpaca and \modeldeepseek. Loss and gradient norm trends are shown separately for each model, along with a comparison of both losses. Figure and results from our work.}
    \label{fig:training_comparison}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                bar width=12pt,
                symbolic x coords={MRR, Hits@1, Hits@3, Hits@10},
                xtick=data,
                ymin=0,
                ymax=0.7,
                ylabel={Score},
                width=12cm,
                height=7cm,
                grid=both,
                major grid style={line width=.2pt, draw=gray!30},
                minor grid style={draw=gray!10},
                enlarge x limits=0.2,
                % Legend placed below with spacing:
                legend style={
                        at={(0.5,-0.15)},
                        anchor=north,
                        /tikz/every even column/.append style={column sep=10pt},
                        legend columns=3,
                        draw=none, % Remove box
                        fill=none,
                        font=\small
                    }
            ]

            % Bars
            \addplot[fill=gray]   coordinates {(MRR,0.549)  (Hits@1,0.474)  (Hits@3,0.585)  (Hits@10,0.641)};
            \addlegendentry{KICGPT}
            \addplot[fill=blue]   coordinates {(MRR,0.3112) (Hits@1,0.1707) (Hits@3,0.3711) (Hits@10,0.5862)};
            \addlegendentry{\modelministral}
            \addplot[fill=purple] coordinates {(MRR,0.2700) (Hits@1,0.1233) (Hits@3,0.2994) (Hits@10,0.6145)};
            \addlegendentry{\modeldeepseek}

        \end{axis}
    \end{tikzpicture}
    \caption{Link Ordering Evaluation Metrics}
    \label{fig:link_ordering}
\end{figure}


\todo{Maybe shortly explain dataset?}
\todo{maybe QUICKLY intro metrics, however should be known already}

\subsection{Concise Model Summaries}

We use three models for our experiments:
Alpaca-7B~\cite{taori2023stanford} is a fine-tuned LLaMA 7B model designed for instruction-following tasks, trained on 52,000 examples generated via OpenAI’s text-davinci-003.
Ministral-8B-Instruct-2410~\cite{mistralai2024ministral8b} is an 8-billion-parameter model fine-tuned for instruction tasks, featuring a 128k context window and multilingual support, outperforming similar-sized models.
Compared to Alpaca, Ministral-8B benefits from a larger model size, an extended context window, and broader multilingual capabilities.
DeepSeek-R1-Distill-Llama-8B~\cite{deepseekai2025deepseekr1distillllama8b} is an 8-billion-parameter model distilled from DeepSeek-R1~\cite{guo2025deepseek}, focusing on reasoning and problem-solving, particularly excelling in math and code tasks.

All experiments were conducted on a system equipped with an NVIDIA RTX A5000 GPU with 24 GiB of VRAM, paired with an Intel Xeon W-1270P CPU running at 3.80 GHz and 64 GB of RAM.



\todo{Performance of KICGPT}

\todo{very unhappy with this, need to write somethign like this}
Following the procedure outlined in~\cref{sec:methodology}, we fine-tuned \modelalpaca on the CoDeX~\cite{safavi2020codex} dataset.
This process took 10 hours and 12 minutes, with the model converging as expected.
The training loss, shown in the leftmost plot of \cref{fig:training_comparison}, follows a bit noisy downward trend, while the gradient norm exhibits significant fluctuations but also trends downards over time.

Similarly, \modeldeepseek was fine-tuned under the same setup, with its training loss shown in the middle plot of \cref{fig:training_comparison}.
9h and 56 minutes.
The loss curve behaves similarly to \modelalpaca, confirming stable convergence, though the gradient norm remains similarly volatile throughout training.
The decreasing trend in gradient norm suggests that while updates remain unstable at each step, their overall magnitude diminishes as training progresses, indicating a gradual refinement of the optimization process.

The final comparison, shown in the rightmost plot of \cref{fig:training_comparison}, demonstrates that both models achieve nearly identical loss trajectories.
So we would expect them to perform similarly.

\todo{end of previous todo}

\todo{evaluations results}

52min alpa
56min r1

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                bar width=12pt,
                symbolic x coords={Accuracy, Precision, Recall, F1-score},
                xtick=data,
                ymin=0,
                ymax=1,
                ylabel={Score},
                width=12cm,
                height=7cm,
                grid=both,
                major grid style={line width=.2pt, draw=gray!30},
                minor grid style={draw=gray!10},
                enlarge x limits=0.2,
                legend style={
                        at={(0.5,-0.15)},
                        anchor=north,
                        /tikz/every even column/.append style={column sep=10pt},
                        legend columns=3,
                        draw=none,
                        fill=none,
                        font=\small
                    }
            ]

            % Bars
            \addplot[fill=gray]   coordinates {(Accuracy,0.8274)  (Precision,0.7791)  (Recall,0.9141)  (F1-score,0.8411)};
            \addlegendentry{KoPA}

            \addplot[fill=blue]   coordinates {(Accuracy,0.5027) (Precision,0.5014) (Recall,0.9759) (F1-score,0.6625)};
            \addlegendentry{\modeldeepseek}

            \addplot[fill=purple] coordinates {(Accuracy,0.5465) (Precision,0.5245) (Recall,0.9962) (F1-score,0.6872)};
            \addlegendentry{\modelalpaca}
        \end{axis}
    \end{tikzpicture}
    \caption{CoDeX Performance Evaluation Metrics}
    \label{fig:codex_evaluation}
\end{figure}

% ---------------------------------------------------------
%
% DISCUSSION
%
% ---------------------------------------------------------


\section{Discussion}\label{sec:discussion}
\todo{TBD, only do if there is actual deeper meaning otherwise fuse with experiments}

\todo{Interpret the results in a deeper way:
    - Why did certain methods do better?
    - Mention any error analysis or interesting observations.
}

The link prediction performance of \modeldeepseek, \modelministral, and ChatGPT varies due to differences in inference behavior, decoding strategies, and instruction adherence.

\modeldeepseek struggled to follow formatting constraints despite explicit prompts. It often produced verbose reasoning before generating answers and self-trimmed responses, sometimes omitting correct entities. This suggests a pretraining bias favoring exhaustive analysis, which conflicted with structured output requirements. Its decoding strategy prioritized information compression, reducing MRR and Hits@1 scores. The few-shot prompting setup may have worsened its tendency to "overthink," making precise, structured outputs harder to achieve.

\modelministral performed better at instruction adherence and structured output than \modeldeepseek. It balanced reasoning depth with clarity but lacked ChatGPT's reranking optimizations, leading to lower MRR and weaker top-ranked predictions. While it ranked entities more consistently, its Hits@10 score lagged slightly, indicating fewer retrieved correct entities overall. Nonetheless, its superior instruction-following made it more effective than \modeldeepseek for structured tasks.

ChatGPT outperformed both models, excelling in instruction adherence and ranking precision due to RLHF and advanced reranking heuristics. It avoided \modeldeepseek's verbosity and \modelministral's retrieval inefficiencies, yielding higher MRR, Hits@1, and Hits@3 scores. Its ability to dynamically contextualize information in few-shot settings further improved entity predictions, making it the most effective model for knowledge graph completion.

% ---------------------------------------------------------
%
% CONCLUSION
%
% ---------------------------------------------------------


\section{Conclusion and Future Work}\label{sec:conclusion-and-future-work}
\todo{TBD otherwise leave out}
\todo{Summarize the key findings and how they link back to your problem statement. Suggest improvements or next steps (e.g., new dataset, advanced fusion technique, or GNN-based approach). Highlight what you learned and what remains challenging.}

\bibliography{main}

\end{document}
