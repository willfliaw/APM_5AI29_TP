\documentclass[12pt,a4paper]{article}

% Quelques options d'encodage, de langues et de format du document
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[top=2cm, bottom=3cm, left=1.75cm, right=1.75cm]{geometry}
\usepackage{setspace}

\usepackage{graphicx} % Pour la commande "\includegraphics"
\usepackage[ % Modified parameters
    bookmarks,
    colorlinks,
    citecolor=black,
    urlcolor=blue,
    linkcolor=black,
    pdfpagemode=UseNone
]{hyperref} % Pour la commande "\url"

\pagenumbering{arabic}

% ------------------------------------------------------------------
% CITATION MANAGEMENT
% ------------------------------------------------------------------

% APA Style
\usepackage{cite}
%\bibliographystyle{apalike}
\bibliographystyle{ieeetr}

\renewcommand\refname{Bibliography}

% Proper formatting and line breaking for URLs
\usepackage{url}

% Change "References" to "Bibliography"
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}{\section*{Bibliography}}{}{}

% ------------------------------------------------------------------
% SUBFIGURE MANAGEMENT
% ------------------------------------------------------------------

\usepackage[caption=false]{subfig}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
\pgfplotsset{compat=1.18}
\usepackage{booktabs}

% ------------------------------------------------------------------
% COLORED TEXT MANAGEMENT
% ------------------------------------------------------------------

\usepackage{xcolor}
\usepackage{todonotes}
\setuptodonotes{inline=always}
\setlength {\marginparwidth }{2cm}

% ------------------------------------------------------------------
% CODE MANAGEMENT
% ------------------------------------------------------------------

\usepackage{listings}

% needs sheel escape activated
% --shell-escape
%\usetikzlibrary{external}
%\tikzexternalize[prefix=figures_tikz_compiled/]

\lstdefinelanguage{Cypher}{
    morekeywords={
        MATCH, RETURN, WHERE, CREATE, DELETE, SET, MERGE, DETACH,
        WITH, LIMIT, SKIP, ORDER, BY, DESC, ASC, OPTIONAL, CALL
    },
    sensitive=true,
    morecomment=[l]{//},
    morecomment=[s]{/*}{*/},
    morestring=[b]',
    morestring=[b]"
}

% Listing style for better readability
\lstset{
    language=Cypher,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    stringstyle=\color{orange},
    commentstyle=\color{gray},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    frame=single
}

\usepackage{xspace}
\usepackage{amsmath}
\usepackage{cleveref}

\usepackage{amsfonts}
\newcommand{\modelministral}{Ministral-8B\xspace}
\newcommand{\modelalpaca}{Alpaca-7B\xspace}
\newcommand{\modeldeepseek}{R1-Distill-8B\xspace}


\begin{document}

\begin{center}
    \begin{tabular}{|p{0.2\textwidth}|p{0.75\textwidth}|}
        \hline
        {
            \vspace{0cm} % without it, bugs, don't know why?
            \centerline{\includegraphics[width=\linewidth]{./images/tp-ipp}}
        }
         & {
                \vspace{0cm} % same here
                \centering
                \large
                {\hfill February, 2025}

                \vspace*{.5cm}
                \textbf{APM\_5AI29\_TP}

                \vspace*{.5cm}
                \setstretch{1.5}
                {\Large\textbf{Language Models and Structured Data}}

                \vspace*{.5cm}
                Final Project Report

                \vspace*{1cm}
        }    \\
        \hline
    \end{tabular}
\end{center}

\noindent Acronym of the Team: AWESome\\
Names: Mochamad Ardiansyah Nurgaha; William Liaw; Eddie Groh; Sriraam Appakutti Palani

    {\centering\rule{\linewidth}{.5pt}}

\begin{center}
    \section*{Knowledge Graph Completion}
\end{center}

% ---------------------------------------------------------
%
% Abstract
%
% ---------------------------------------------------------

\section{Abstract}

This work explores Knowledge Graph Completion (KGC) using two similar approaches: prompt-based tail entity ranking and fine-tuning-based triple classification.
By leveraging structured retrieval for ranking candidates and integrating knowledge-infused prompts for classification, we assess the effectiveness of different large language models across various KGC tasks.

% ---------------------------------------------------------
%
% PROBLEM STATEMENT
%
% ---------------------------------------------------------

\section{Problem Statement}\label{sec:problem-statement}

Knowledge Graph Completion aims to infer missing facts in a knowledge graph (KG) by leveraging existing structural and semantic relationships.
A knowledge graph consists of entities, relations, and triples of the form \( (h, r, t) \), where \( h \) (head) and \( t \) (tail) are connected by relation \( r \).
Due to the inherent incompleteness of KGs, KGC seeks to either predict missing entities or verify candidate triples.

Although KGC is often framed as a single task, different methodological approaches necessitate distinct problem formulations.
In this work, we consider two complementary tasks: \emph{link prediction} and \emph{triple classification}.
This distinction arises from the differing nature of the evaluated approaches.
While they could be adapted to a unified setting, doing so would require substantial modifications that could alter their original design, and we therefore refrained from doining so at the cost of having two slightly different problem formulations.

Although KGC is often framed as a single task, different methodological approaches necessitate distinct problem formulations.
In this work, we focus on two complementary tasks: \emph{link prediction} and \emph{triple classification}.
This distinction reflects the differences in the evaluated approaches.
While it is certainly possible to adapt both approaches to a unified setting, doing so would require significant modifications that could change their original design.
To preserve the intended methodology from the papers, we therefore opted to maintain these two slightly different problem formulations.

\paragraph{Link Prediction:}
Link prediction involves inferring a missing entity in an incomplete triple. Given \( (h, r, ?) \) or \( (?, r, t) \), the goal is to rank candidate entities based on their plausibility as a valid completion.

\paragraph{Triple Classification:}
Triple classification assesses whether a given triple \( (h, r, t) \) is valid. Instead of ranking candidates, the model outputs a binary decision indicating whether the relationship holds.

% ---------------------------------------------------------
%
% BACKGROUND
%
% ---------------------------------------------------------


\section{Background}\label{sec:background}

Early KGC research centered on triple-based models using embedding methods to represent entities and relations in a continuous vector space.
TransE~\cite{bordes2013translating} models relations as translation operations, while DistMult~\cite{yang2014embedding} and ComplEx~\cite{trouillon2016complex} apply tensor factorization, the latter extending to complex-valued embeddings.

%Advancements introduced Graph Neural Networks (GNNs)~\cite{schlichtkrull2018modeling} to refine entity representations by aggregating structural information.
%However, these models struggle with long-tail entities due to sparse connectivity, limiting predictive performance.

%To overcome these limitations, text-based KGC leverages pre-trained language models (PLMs) to incorporate entity and relation descriptions. KG-BERT~\cite{yao2019kgbert} reframes KGC as a classification problem, encoding triples as text for distinguishing valid from invalid ones.
%Despite effectiveness, these methods require extensive fine-tuning and high computational costs, restricting scalability.

Large Language Models (LLMs) have revolutionized KGC by leveraging their broad internal knowledge and advanced reasoning capabilities.
This enables, in some cases, the ability to perform link prediction without even being trained on the target data, using techniques such as in-context learning and instruction tuning.
Unlike traditional KG models, which rely on structured embeddings, LLMs offer greater flexibility for zero-shot and few-shot predictions.
However, they often struggle with hallucination and structural misalignment with KG schemas.


We divide LLM-based KGC solutions into two main categories:

\paragraph{Purely Prompt-Based Methods:} These exploit structured retrieval or textual descriptions alongside in-context learning, without modifying LLM parameters.
For instance, MPIKGC~\cite{xu2024mpikgc} enhances description-based KGC models by querying LLMs to expand entity representations, thereby improving relation understanding and capturing structural clues.
KICGPT~\cite{wei2023kicgpt} stands out by incorporating a triple-based retriever for preliminary ranking, then refining the candidate list through \emph{Knowledge Prompts} that encode KG structure directly into the LLM's prompts.
Although KICGPT's approach remains parameter-free, it injects structural awareness more explicitly than simpler prompt-based methods.

\paragraph{Fine-Tuning-Based Methods:} These methods embed KG structure into the LLM through additional training.
For example, DIFT~\cite{liu2024dift} refines the model with discrimination instructions to better identify valid entities, while KC-GenRe~\cite{wang2024kcgenre} adopts a knowledge-constrained re-ranking technique, updating LLM weights to emphasize entity ranking.
%RPKGC~\cite{khalil2024rpkgc} fine-tunes a model for relation prediction, learning to infer plausible connections between entity pairs.
Finally, KoPA~\cite{qin2023kopa} integrates a \emph{Knowledge Prefix Adapter}, mapping KG embeddings into the LLM's textual space to guide inference with structural signals.
Although KoPA modifies model weights, it keeps the KG structure directly accessible during generation and prediction.


% ---------------------------------------------------------
%
% METHODOLOGY
%
% ---------------------------------------------------------


\section{Methodology}\label{sec:methodology}

Following the analysis in \cref{sec:background}, we now outline our methodological approach to KGC. Our work examines one representative method from each of the previously discussed categories: KICGPT, a purely prompt-based approach for tail prediction, and KoPA, which applies fine-tuning for triple classification.


\subsection{KICGPT: Prompt-Based Link Prediction}
\label{sec:method:kicgpt}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/KICGPTarchitecture}
    \caption{KICGPT architecture, consisting of a triple-based KGC retriever, the \emph{Knowledge Prompt}, and an LLM-based reranker.}
    \label{fig:KICGPTarchitecture}
\end{figure}

KICGPT primarily consists of three components, as illustrated in \cref{fig:KICGPTarchitecture}: a triple-based KGC retriever, a \emph{Knowledge Prompt} module, and an LLM-based reranker.
Given a query triple \( (h, r, ?) \) from a KG \(G\) consisting of set of entities \(E\) and relations \(R\), the retriever computes a ranking over all possible entities \( e \in E \), generating an initial ordered list \( R_{\text{retriever}} = [e_1, e_2, ..., e_{|E|}] \) based on a predefined scoring function. The \emph{Knowledge Prompt} then provides structured in-context learning examples to guide the LLM, which subsequently refines the top-\( m \) candidates, producing a reranked list \( R_{\text{LLM}} = [e'_1, e'_2, ..., e'_m] \).
The final output of KICGPT combines the LLM’s refined ranking with the remaining entities from the retriever, yielding the final entity ranking:

\[
    R_{\text{KICGPT}} = [e'_1, e'_2, ..., e'_m, e_{m+1}, ..., e_{|E|}].
\]

\subsubsection{Knowledge Prompt Construction}

The \emph{Knowledge Prompt} enables KICGPT to leverage in-context learning by integrating relevant KG information into the LLM’s input.
The prompt is constructed from two pools of triples: the analogy pool \( D_a \) and the supplement pool \( D_s \).
The analogy pool consists of triples that share the same relation \( r \) as the query, ensuring that the LLM is exposed to examples with similar relational structures:

\[
    D_a = \{(e', r, e'') \in G \mid e', e'' \in E\}.
\]

The supplement pool contains triples where the head or tail entity matches the query's head entity \( h \), providing additional entity-specific context:

\[
    D_s = \{(h, r', e') \in G \mid r' \in R, e' \in E\} \cup \{(e', r', h) \in G \mid r' \in R, e' \in E\}.
\]

To ensure that the most relevant demonstrations are selected, triples in \( D_s \) are ranked using BM25 scores, which measure textual similarity to the query entity description.
The final ordered sets, denoted as \( L_a \) for analogy-based examples and \( L_s \) for supplement-based examples, are constructed to optimize the informativeness of the prompt.
For the analogy pool, entity counters are initialized at zero, and triples are iteratively selected to maximize diversity while minimizing repetition.

\subsubsection{Prompt Engineering and Query Processing}

To effectively incorporate the selected examples, KICGPT transforms structured triples into natural language statements using a unified prompt template.
This ensures consistent formatting across queries and demonstrations, making the input more interpretable for the LLM. The prompt follows a structured process consisting of the following phases:

\begin{itemize}
    \item \textbf{Task Description:} The prompt begins with a responsibility statement that explicitly clarifies the LLM’s role in ranking candidate entities based on relational context.
    \item \textbf{Query Introduction:} The incomplete triple \( (h, r, ?) \) is presented, ensuring the model understands the missing component it must predict.
    \item \textbf{Demonstration Explanation:} The two sources of contextual examples, analogy-based triples \( L_a \) and supplement-based triples \( L_s \), are described so that the LLM can recognize their significance in guiding predictions.
    \item \textbf{Demonstration Integration:} Selected triples from \( L_a \) and \( L_s \) are appended to the prompt. The number of demonstrations is maximized within the LLM’s token limit to provide sufficient context.
    \item \textbf{Re-Ranking and Final Prediction:} The LLM processes the structured prompt and re-ranks the top-\( m \) candidates, producing an ordered list \( R_{\text{LLM}} \). The highest-ranked entities from this list replace the corresponding top-\( m \) results from the retriever, yielding the final ranking.
\end{itemize}


\subsection{KoPA: Fine-Tuning-Based Triple Classification}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/KoPAarchitecture}
    \caption{KoPA architecture, illustrating the integration of structural embeddings with an LLM via the Knowledge Prefix Adapter.}
    \label{fig:KoPAarchitecture}
\end{figure}

KoPA addresses the triple classification problem by embedding structured knowledge graph representations into the LLM’s reasoning process, explicitly incorporating KG structure through fine-tuning rather than relying on prompt-based ranking.
By leveraging structured KG embeddings, KoPA enhances the LLM’s ability to distinguish between valid and invalid triples, integrating relational constraints directly into its decision-making process.
As illustrated in \cref{fig:KoPAarchitecture}, the model combines structural embeddings with a knowledge prefix adapter, enabling a seamless fusion of KG-based constraints with LLM inference.

\subsubsection{Structural Embeddings}
To integrate KG structure, each entity \( e \) and relation \( r \) is mapped into a latent vector space, where relational dependencies are preserved. The learned embeddings, denoted as \( \mathbf{e} \in \mathbb{R}^{d_e} \) for entities and \( \mathbf{r} \in \mathbb{R}^{d_r} \) for relations, capture topological and semantic information from the KG. A scoring function \( F(h, r, t) = \mathbf{h}^\top \mathbf{W_r} \mathbf{t} \), where \( \mathbf{h}, \mathbf{t} \) are entity embeddings and \( \mathbf{W_r} \) is a relation-specific transformation matrix, is used to estimate the plausibility of a triple.

These embeddings are pre-trained using self-supervised learning, ensuring that relational constraints are reflected in the representation space.
Unlike textual descriptions, which can be noisy or incomplete, structured embeddings provide a more direct way of modeling entity relationships.
By aligning embeddings with the LLM’s input, KoPA ensures that structural information is explicitly available during inference.

\subsubsection{Knowledge Prefix Adapter}

To bridge structured embeddings with the LLM’s textual processing, KoPA employs a \textit{Knowledge Prefix Adapter} (KPA). This adapter transforms structured embeddings into a virtual prefix, effectively mapping entity and relation representations into the model’s input space. Given a triple \( (h, r, t) \), the adapter projects its corresponding embeddings \( \mathbf{h}, \mathbf{r}, \mathbf{t} \) into a sequence of virtual tokens, denoted as \( K = P(\mathbf{h}) \oplus P(\mathbf{r}) \oplus P(\mathbf{t}) \), where \( P \) is a learned transformation function.

The knowledge prefix is prepended to the LLM’s textual input, allowing the model to condition its predictions on structured KG information.
This approach ensures that the model retains relational constraints while maintaining flexibility in handling natural language inputs.
Unlike fully fine-tuned KG embeddings, which modify the LLM’s internal parameters, the prefix adapter enables lightweight structural integration without overfitting to specific KG schemas.

\subsubsection{Fine-Tuning and Classification}
% KoPA is trained on a dataset of valid and invalid triples to optimize its ability to distinguish factual from incorrect knowledge.
% Given an input triple \( (h, r, t) \), the model computes a classification score \( g(h, r, t) \), indicating the likelihood that the triple is valid.
% The knowledge prefix adapter ensures that the LLM considers both explicit KG structure and learned textual reasoning when making classification decisions.
% This hybrid approach leverages the strengths of structured embeddings while retaining the adaptability of large-scale LLMs.

The knowledge prefix and the textual input are jointly processed by the LLM,
which predicts the validity of the input triple based on the combined information.
The LLM is fine-tuned on the instruction-following task to optimize its triple classification accuracy.
% ---------------------------------------------------------
%
% EXPERIMENTS
%
% ---------------------------------------------------------


\section{Experiments}\label{sec:experiments}

\begin{figure}
    \centering
    \begin{tikzpicture}
        % Group plot configuration
        \begin{groupplot}[
                group style={
                        group size=3 by 1,       % 3 plots in 1 row
                        horizontal sep=0.3cm     % Adjust spacing between plots
                    },
                width=0.38\textwidth,        % Maximized plot width
                ymode=log,                   % Log-scale y-axis for ALL plots
                xmin=0, xmax=3,              % Unified x-axis range
                ymin=1e-3, ymax=1,           % Unified y-axis range across all plots
                xlabel=Epoch,
                xtick distance=0.5,          % Uniform tick spacing
                ytick distance=10,           % Log-scale tick separation
                minor y tick num=9,          % Minor tick marks for readability
            ]

            %------------- Plot (a) Alpaca -----------------
            \nextgroupplot[
                title=\modelalpaca,
                % Hide y-axis labels for alignment
                ylabel=Loss
            ]
            \addplot[smooth, thick, blue]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-Llama-2-7b-alpaca-cleaned-finetune.dat};

            \addplot[smooth, thick, cyan, dashed]
            table[x=epoch, y=grad-norm, col sep=space]
                {figures/raw-data/lora-Llama-2-7b-alpaca-cleaned-finetune.dat};


            %------------- Plot (b) DeepSeek ---------------
            \nextgroupplot[
                title=\modeldeepseek,
                yticklabels={,,}
            ]
            \addplot[smooth, thick, purple]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-DeepSeek-R1-Distill-Llama-8B-finetune.dat};

            \addplot[smooth, thick, violet, dashed]
            table[x=epoch, y=grad-norm, col sep=space]
                {figures/raw-data/lora-DeepSeek-R1-Distill-Llama-8B-finetune.dat};

            %------------- Plot (c) Loss Comparison -------------
            \nextgroupplot[
                title=Loss Comparison,
                yticklabels={,,}   % Hide y-axis tick labels for uniform look
            ]
            \addplot[smooth, thick, purple]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-DeepSeek-R1-Distill-Llama-8B-finetune.dat};

            \addplot[smooth, thick, blue]
            table[x=epoch, y=loss, col sep=space]
                {figures/raw-data/lora-Llama-2-7b-alpaca-cleaned-finetune.dat};

        \end{groupplot}
    \end{tikzpicture}

    % Description of colors with more spacing in legend
    \vspace{0.2cm}
    {\centering
        \begin{tikzpicture}
            \node[draw, fill=blue, minimum width=0.4cm, minimum height=0.4cm] (A) {};
            \node[left=0.3cm of A] {\modelalpaca:};
            \node[right=0.05cm of A] {Loss};

            \node[draw, fill=cyan, minimum width=0.4cm, minimum height=0.4cm, right=1.5cm of A] (B) {};
            \node[right=0.05cm of B] {Gradient Norm};

            \node[draw, fill=purple, minimum width=0.4cm, minimum height=0.4cm, below=0.2cm of A] (C) {};
            \node[left=0.3cm of C] {\modeldeepseek:};
            \node[right=0.05cm of C] {Loss};

            \node[draw, fill=violet, minimum width=0.4cm, minimum height=0.4cm, right=1.5cm of C] (D) {};
            \node[right=0.05cm of D] {Gradient Norm};
        \end{tikzpicture}
    }

    \caption{Fine-tuning of \modelalpaca and \modeldeepseek. Loss and gradient norm trends are shown separately for each model, along with a comparison of both losses. Figure and results from our work.}
    \label{fig:training_comparison}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                bar width=12pt,
                symbolic x coords={MRR, Hits@1, Hits@3, Hits@10},
                xtick=data,
                ymin=0,
                ymax=0.7,
                ylabel={Score},
                width=12cm,
                height=7cm,
                grid=both,
                major grid style={line width=.2pt, draw=gray!30},
                minor grid style={draw=gray!10},
                enlarge x limits=0.2,
                % Legend placed below with spacing:
                legend style={
                        at={(0.5,-0.15)},
                        anchor=north,
                        /tikz/every even column/.append style={column sep=10pt},
                        legend columns=4,
                        draw=none, % Remove box
                        fill=none,
                        font=\small
                    }
            ]

            % Bars
            \addplot[fill=gray]   coordinates {(MRR,0.549)  (Hits@1,0.474)  (Hits@3,0.585)  (Hits@10,0.641)};
            \addlegendentry{KICGPT}
            \addplot[fill=teal]   coordinates {(MRR,0.3112) (Hits@1,0.1707) (Hits@3,0.3711) (Hits@10,0.5862)};
            \addlegendentry{\modelministral}
            \addplot[fill=purple] coordinates {(MRR,0.2700) (Hits@1,0.1233) (Hits@3,0.2994) (Hits@10,0.6145)};
            \addlegendentry{\modeldeepseek}

            \addplot[fill=violet] coordinates {(MRR,0.2461) (Hits@1,0.0980) (Hits@3,0.2580) (Hits@10,0.6120)};
            \addlegendentry{\modeldeepseek Zeroshot}


        \end{axis}
    \end{tikzpicture}
    \caption{Link Ordering Evaluation Metrics}
    \label{fig:link_ordering}
\end{figure}


\subsection{Datasets}

For our experiments, we used two widely adopted KGC benchmarks, which are summarized in \cref{tab:datasets}.

\paragraph{WN18RR} is a filtered version of WN18~\cite{wn18rr}, derived from WordNet~\cite{wordnet}, designed to eliminate inverse relations that previously made link prediction trivial.
This refinement increases task difficulty, requiring models to generalize beyond simple pattern recognition.

\paragraph{CoDeX-S} (Complex Dataset Extracted from Wikidata~\cite{wikidata}) is a subset of CoDeX~\cite{safavi2020codex}, offering a diverse set of relational facts.
Unlike WN18RR, which focuses on lexical semantics, CoDeX-S is sourced from Wikidata, ensuring a richer relational structure with a more balanced distribution of relations.

\begin{table}
    \centering
    \begin{tabular}{l c c c c c}
        \hline
        Dataset & \#Entities & \#Relations & \#Train & \#Test \\
        \hline
        WN18RR & 40,943 & 11 & 86,835 & 3,134 \\
        CoDeX-S & 2,034 & 42 & 32,888 & 3,654 \\
        \hline
    \end{tabular}
    \caption{Statistics of the datasets used in our experiments.}
    \label{tab:datasets}
\end{table}

\subsection{Language Models}

To evaluate model performance across different architectures, we consider three language models with distinct capabilities:

\paragraph{Alpaca-7B}~\cite{taori2023stanford} is a fine-tuned variant of LLaMA 7B, trained on 52,000 instruction-response pairs generated by OpenAI’s text-davinci-003.
Optimized for general instruction-following, it serves as a lightweight baseline.

\paragraph{Mistral-8B-Instruct-2410}~\cite{mistralai2024ministral8b} is an 8-billion-parameter model designed for instruction-based tasks, featuring a 128k-token context window and multilingual support.
Compared to Alpaca-7B, it benefits from a slightly larger model size, extended context length, and improved cross-lingual capabilities.

\paragraph{DeepSeek-R1-Distill-Llama-8B}~\cite{deepseekai2025deepseekr1distillllama8b} is a distilled version of DeepSeek-R1~\cite{guo2025deepseek}, optimized for reasoning and problem-solving.
With a strong emphasis on mathematical and coding tasks, it is particularly effective in structured problem domains.


\subsection{Evaluation of the KICGPT Approach}

All experiments were conducted on a system equipped with an NVIDIA RTX A5000 GPU with 24 GiB of VRAM, an Intel Xeon W-1270P CPU running at 3.80 GHz, and 64 GB of RAM.

As previously discussed, this approach does not require modifications to the models themselves.
However, while the original implementation relied on the ChatGPT API, we adapted it to locally running models.
Specifically, in \cref{fig:link_ordering} we evaluated \modelministral and \modeldeepseek, comparing their performance against the results reported by Wei et al.~\cite{wei2023kicgpt}.

\modelministral followed prompt instructions closely and rarely produced unparsable output.
However, it significantly underperformed compared to the original study,
which we tentatively attribute to ChatGPT-3.5’s superior general knowledge and reasoning capabilities.

\modeldeepseek demonstrated strong reasoning abilities inside its explicit thinking tags, effectively analyzing given options, recognizing patterns, and identifying preferable choices.
However, this process was often excessively verbose, even when explicitly instructed to be concise.
Additionally, it frequently truncated its final responses, replacing parts of the output with ellipses (\dots), likely due to a focus loss while reasoning about the problem.
Moreover, \modeldeepseek sometimes disregarded specific prompt instructions, potentially because it is primarily tuned for mathematical and coding tasks.
Since the full-size Deepseek-R1 model is known to suffer from degraded performance with few-shot prompting~\cite{guo2025deepseek}, a similar issue may affect this distilled variant.
However, as the few-shot examples contain domain-specific information, removing them entirely could  also further reduce performance.

Both models performed surprisingly on-par with ChatGPT in the Hits@10 evaluation.
However, this is likely due to variations in output length rather than genuine ranking improvements.
Since evaluation is based on the options generated by the LLM rather than the full ground-truth ranking, models that produce shorter responses inherently increase their chances of including the correct answer among the top-ranked candidates.


\subsection{Evaluation of the KoPA Approach}

Following the procedure outlined in~\cref{sec:methodology}, we fine-tuned \modelalpaca on the CoDeX~\cite{safavi2020codex} dataset.
The process took 10 hours and 12 minutes, with the model converging as expected.
As shown in \cref{fig:training_comparison}, the training loss follows a slightly noisy downward trend, while the gradient norm exhibits significant fluctuations but generally decreases over time.

Using the same setup, \modeldeepseek was fine-tuned in 9 hours and 56 minutes.
Its loss curve closely resembles that of \modelalpaca, confirming stable convergence, though the gradient norm remains similarly volatile.
A direct comparison reveals that both models exhibit nearly identical loss trajectories, suggesting comparable performance.

This expectation is confirmed in \cref{fig:codex_evaluation}, where both models achieve similar results.
Evaluation times were 52 minutes for \modelalpaca and 56 minutes for \modeldeepseek.
However, both models lag behind the performance reported in the original paper, though the cause remains unclear.
Notably, they heavily favor predicting \textit{True}, resulting in exceptionally high recall but lower precision.

An important limitation of this fine-tuning approach is that \modeldeepseek does not engage in reasoning, as it is trained to generate the correct answer in its first token.
Allowing it to reason could potentially improve performance, but this would require a different fine-tuning methodology.


\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                ybar,
                bar width=12pt,
                symbolic x coords={Accuracy, Precision, Recall, F1-score},
                xtick=data,
                ymin=0,
                ymax=1,
                ylabel={Score},
                width=12cm,
                height=7cm,
                grid=both,
                major grid style={line width=.2pt, draw=gray!30},
                minor grid style={draw=gray!10},
                enlarge x limits=0.2,
                legend style={
                        at={(0.5,-0.15)},
                        anchor=north,
                        /tikz/every even column/.append style={column sep=10pt},
                        legend columns=3,
                        draw=none,
                        fill=none,
                        font=\small
                    }
            ]

            % Bars
            \addplot[fill=gray]   coordinates {(Accuracy,0.8274)  (Precision,0.7791)  (Recall,0.9141)  (F1-score,0.8411)};
            \addlegendentry{KoPA}

            \addplot[fill=blue]   coordinates {(Accuracy,0.5027) (Precision,0.5014) (Recall,0.9759) (F1-score,0.6625)};
            \addlegendentry{\modeldeepseek}

            \addplot[fill=purple] coordinates {(Accuracy,0.5465) (Precision,0.5245) (Recall,0.9962) (F1-score,0.6872)};
            \addlegendentry{\modelalpaca}
        \end{axis}
    \end{tikzpicture}
    \caption{CoDeX Performance Evaluation Metrics}
    \label{fig:codex_evaluation}
\end{figure}

% ---------------------------------------------------------
%
% CONCLUSION
%
% ---------------------------------------------------------


\section{Conclusion and Future Work}\label{sec:conclusion-and-future-work}
\todo{TBD otherwise leave out}
\todo{Summarize the key findings and how they link back to your problem statement. Suggest improvements or next steps (e.g., new dataset, advanced fusion technique, or GNN-based approach). Highlight what you learned and what remains challenging.}

\bibliography{main}

\end{document}
