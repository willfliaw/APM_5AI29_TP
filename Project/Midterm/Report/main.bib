@article{hetionet,
  article_type = {journal},
  title        = {Systematic integration of biomedical knowledge prioritizes drugs for repurposing},
  author       = {Himmelstein, Daniel Scott and Lizee, Antoine and Hessler, Christine and Brueggeman, Leo and Chen, Sabrina L and Hadley, Dexter and Green, Ari and Khankhanian, Pouya and Baranzini, Sergio E},
  editor       = {Valencia, Alfonso},
  volume       = 6,
  year         = 2017,
  month        = {sep},
  pub_date     = {2017-09-22},
  pages        = {e26726},
  citation     = {eLife 2017;6:e26726},
  doi          = {10.7554/eLife.26726},
  url          = {https://doi.org/10.7554/eLife.26726},
  abstract     = {The ability to computationally predict whether a compound treats a disease would improve the economy and success rate of drug approval. This study describes Project Rephetio to systematically model drug efficacy based on 755 existing treatments. First, we constructed Hetionet (neo4j.het.io), an integrative network encoding knowledge from millions of biomedical studies. Hetionet v1.0 consists of 47,031 nodes of 11 types and 2,250,197 relationships of 24 types. Data were integrated from 29 public resources to connect compounds, diseases, genes, anatomies, pathways, biological processes, molecular functions, cellular components, pharmacologic classes, side effects, and symptoms. Next, we identified network patterns that distinguish treatments from non-treatments. Then, we predicted the probability of treatment for 209,168 compound–disease pairs (het.io/repurpose). Our predictions validated on two external sets of treatment and provided pharmacological insights on epilepsy, suggesting they will help prioritize drug repurposing candidates. This study was entirely open and received realtime feedback from 40 community members.},
  keywords     = {drug repurposing, heterogeneous networks, machine learning},
  journal      = {eLife},
  issn         = {2050-084X},
  publisher    = {eLife Sciences Publications, Ltd}
}

@misc{neo4j,
  title    = {NEO4J Graph Database \& Analytics | Graph Database Management System},
  url      = {https://neo4j.com/},
  journal  = {Graph Database & Analytics},
  author   = {Neo4j},
  year     = {2024},
  month    = jul,
  language = {en-US}
}

@article{pykeen,
  author  = {Ali, Mehdi and Berrendorf, Max and Hoyt, Charles Tapley and Vermue, Laurent and Sharifzadeh, Sahand and Tresp, Volker and Lehmann, Jens},
  journal = {Journal of Machine Learning Research},
  number  = {82},
  pages   = {1--6},
  title   = {{PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings}},
  url     = {http://jmlr.org/papers/v22/20-825.html},
  volume  = {22},
  year    = {2021}
}

@misc{sun2019rotateknowledgegraphembedding,
      title={RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space}, 
      author={Zhiqing Sun and Zhi-Hong Deng and Jian-Yun Nie and Jian Tang},
      year={2019},
      eprint={1902.10197},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.10197}, 
}

@inproceedings{10.5555/2999792.2999923,
author = {Bordes, Antoine and Usunier, Nicolas and Garcia-Dur\'{a}n, Alberto and Weston, Jason and Yakhnenko, Oksana},
title = {Translating embeddings for modeling multi-relational data},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We consider the problem of embedding entities and relationships of multi-relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2787–2795},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{10.5555/2893873.2894046,
author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
title = {Knowledge graph embedding by translating on hyperplanes},
year = {2014},
publisher = {AAAI Press},
abstract = {We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1112–1119},
numpages = {8},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}

@inproceedings{10.5555/2886521.2886624,
author = {Lin, Yankai and Liu, Zhiyuan and Sun, Maosong and Liu, Yang and Zhu, Xuan},
title = {Learning entity and relation embeddings for knowledge graph completion},
year = {2015},
isbn = {0262511290},
publisher = {AAAI Press},
abstract = {Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https://github.com/mrlyk423/relation_extraction.},
booktitle = {Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
pages = {2181–2187},
numpages = {7},
location = {Austin, Texas},
series = {AAAI'15}
}

@inproceedings{ji-etal-2015-knowledge,
    title = "Knowledge Graph Embedding via Dynamic Mapping Matrix",
    author = "Ji, Guoliang  and
      He, Shizhu  and
      Xu, Liheng  and
      Liu, Kang  and
      Zhao, Jun",
    editor = "Zong, Chengqing  and
      Strube, Michael",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1067/",
    doi = "10.3115/v1/P15-1067",
    pages = "687--696"
}

@misc{yang2015embeddingentitiesrelationslearning,
      title={Embedding Entities and Relations for Learning and Inference in Knowledge Bases}, 
      author={Bishan Yang and Wen-tau Yih and Xiaodong He and Jianfeng Gao and Li Deng},
      year={2015},
      eprint={1412.6575},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1412.6575}, 
}

@inproceedings{10.5555/3104482.3104584,
author = {Nickel, Maximilian and Tresp, Volker and Kriegel, Hans-Peter},
title = {A three-way model for collective learning on multi-relational data},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {809–816},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{Balazevic_2019,
   title={TuckER: Tensor Factorization for Knowledge Graph Completion},
   url={http://dx.doi.org/10.18653/v1/D19-1522},
   DOI={10.18653/v1/d19-1522},
   booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   publisher={Association for Computational Linguistics},
   author={Balazevic, Ivana and Allen, Carl and Hospedales, Timothy},
   year={2019} }
